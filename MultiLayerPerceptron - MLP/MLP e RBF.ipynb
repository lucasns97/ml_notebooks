{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP e RBF.ipynb","provenance":[],"collapsed_sections":["dypO8NO1vcjO","yzb002xew6m-","Z0amOfaScqyA","_vO1xJRxmHpk","ek9SZGMvMrNk","n6ZGiQnXy3bi","CG3q9s-gsqct","Ohg2D7-AziyD","K5Y58Hcm7bqO","wu7pus6ZHxEf","YsAZVWEi0UIS","yQk6DxT-aX7T","6DO0LEch0P8S","LClU_bH_q6CF","0x5vI2TEr23_"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6823deee41c24c1eb533dd59328d2219":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_abea2ff96a884a6fae096366b62cf212","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_38129aa21e5a4c6c8a3657df76bfa8ad","IPY_MODEL_4b71f585858e4669a3279fb209a5fa6e","IPY_MODEL_102c27228d1e4d7bb0bbfe1b536da782"]}},"abea2ff96a884a6fae096366b62cf212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38129aa21e5a4c6c8a3657df76bfa8ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1ab403d31fec4378a3f34800fb955434","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 500/500: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8e8a23802074747aa896c2a851f50f0"}},"4b71f585858e4669a3279fb209a5fa6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2bece20782994b26807b3aea716d2582","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd99e512ccab496d878644b291403e36"}},"102c27228d1e4d7bb0bbfe1b536da782":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9331beeeadf5484e91712a70e52eb468","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 500/500 [00:24&lt;00:00, 20.93it/s, loss=0.0158, score=0.983]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec19e2e1153b40338634816a24636dbb"}},"1ab403d31fec4378a3f34800fb955434":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8e8a23802074747aa896c2a851f50f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2bece20782994b26807b3aea716d2582":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fd99e512ccab496d878644b291403e36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9331beeeadf5484e91712a70e52eb468":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ec19e2e1153b40338634816a24636dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"26ed656db0ca4742a21cf19e2271330a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ecb054a8f99247e8bab3cc763355ca2f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a574482289ea4582a613917fecefe01e","IPY_MODEL_e5cf8b7e03f4443392719a6659db255f","IPY_MODEL_9f8e5e1287ed43019614a71f99f30480"]}},"ecb054a8f99247e8bab3cc763355ca2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a574482289ea4582a613917fecefe01e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f78ca637e114fd6b0995b6d19d61655","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 1000/1000: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_427f48725d9244189e6515740961822c"}},"e5cf8b7e03f4443392719a6659db255f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_682db4a662784225a016d699d6fce63d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f5297f0df4fd4ed3bfa658fe8b96719a"}},"9f8e5e1287ed43019614a71f99f30480":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b987d4bf0afa4ff082528f3fe7006995","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 997/1000 [00:18&lt;00:00, 48.46it/s, loss=0.221, score=0.817]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_644a0a11ae454f2dbd4695ce07f30dad"}},"9f78ca637e114fd6b0995b6d19d61655":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"427f48725d9244189e6515740961822c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"682db4a662784225a016d699d6fce63d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f5297f0df4fd4ed3bfa658fe8b96719a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b987d4bf0afa4ff082528f3fe7006995":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"644a0a11ae454f2dbd4695ce07f30dad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"8yF82hapsX1s"},"source":["# Exercício 3 - MLP e RBF\n","\n","---------------------------------------------\n","\n","### Participants:\n"," - Francielle Vargas - 9527629\n"," - Lucas Nunes Sequeira - 9009642\n"," - Emanuel Huber - 12110113\n","\n","#### Date: 24/09/2021\n","\n","---------------------------------------------\n","\n","#### Descrição e Instrução\n","\n","Este notebook foi feito para a disciplina SCC5809 - Redes Neurais\n","\n","No notebook contém\n","\n","1. A implementação da classe **MLP**\n","2. A implementação da solução **RBF**\n","3. Utilização dos modelos no dataset **Iris**\n","\n","Para utilizá-lo basta executar todas as células.\n","\n","_link de acesso ao colab: https://colab.research.google.com/drive/1PRfwXYy6K1E1zsKYF6W93L0oiiKamihl?usp=sharing_"]},{"cell_type":"markdown","metadata":{"id":"dypO8NO1vcjO"},"source":["### Libs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYNxDhZBG8rj","executionInfo":{"status":"ok","timestamp":1632524648534,"user_tz":180,"elapsed":14323,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"22b118bd-3b64-4f4b-e2df-f236565dd387"},"source":["# Install Libs\n","!pip install numpy==1.19.5\n","!pip install tqdm==4.62.0\n","!pip install plotly==4.4.1\n","!pip install pandas==1.1.5\n","!pip install scikit-learn==0.24.2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: tqdm==4.62.0 in /usr/local/lib/python3.7/dist-packages (4.62.0)\n","Requirement already satisfied: plotly==4.4.1 in /usr/local/lib/python3.7/dist-packages (4.4.1)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly==4.4.1) (1.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==4.4.1) (1.15.0)\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n","Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (0.24.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.19.5)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (2.2.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"id":"MZu5FYUFvd-0","executionInfo":{"status":"ok","timestamp":1632524649722,"user_tz":180,"elapsed":1191,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["# Math lib\n","import numpy as np\n","\n","# Log lib\n","from tqdm.auto import tqdm\n","\n","# Visualization Lib\n","import plotly.express as px\n","\n","# Copy for deepcopy\n","import copy\n","\n","# Pandas for visualization\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Iris data\n","from sklearn.datasets import load_iris\n","\n","# Split data\n","from sklearn.model_selection import train_test_split\n","\n","# Scaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Metrics\n","from sklearn.metrics import accuracy_score\n","\n","# Feature selection\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.feature_selection import SelectFromModel\n","\n","# Seed\n","SEED = 42\n","\n","# Set random seed\n","np.random.seed(SEED)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzb002xew6m-"},"source":["### Activation function"]},{"cell_type":"code","metadata":{"id":"gjexUbwHw5u3","executionInfo":{"status":"ok","timestamp":1632524649723,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class Sigmoid(object):\n","  '''\n","  Sigmoid Activation Function\n","  \n","    f(x) =  1 / (1 + e^(-x))\n","  '''\n","\n","  def __init__(self):\n","\n","    # Last call data\n","    self.last_grad = 0\n","    self.last_input = 0\n","    self.last_output = 0\n","\n","  def _update_last_call(self, x, y, grad = True):\n","    '''Update last call data'''\n","\n","    # Gradient\n","    if grad:\n","      self.last_grad = self.gradient(x)\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, x):\n","    '''Calculate sigmoid function of x'''\n","\n","    if isinstance(x, list):\n","      x = np.array(x)\n","\n","    return 1 / (1 + np.exp(-x))\n","\n","  def calculate(self, x, grad: bool = True):\n","    '''Calculate sigmoid function of x'''\n","\n","    # Calculation\n","    y = self(x)\n","\n","    # Update last call data\n","    self._update_last_call(x, y, grad)\n","\n","    return y\n","\n","  def copy(self):\n","    return Sigmoid()\n","\n","  def gradient(self, x):\n","    '''Calculate sigmoid gradient within x'''\n","\n","    return self(x)*(1 - self(x))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCUDAXp8Wfwg","executionInfo":{"status":"ok","timestamp":1632524649723,"user_tz":180,"elapsed":7,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class Swish(object):\n","  '''\n","  Swish Activation Function\n","  \n","    f(x) =  x * sigmoid(x)\n","  '''\n","\n","  def __init__(self):\n","\n","    self.sigmoid = Sigmoid()\n","\n","    # Last call data\n","    self.last_grad = 0\n","    self.last_input = 0\n","    self.last_output = 0\n","\n","  def _update_last_call(self, x, y, grad = True):\n","    '''Update last call data'''\n","\n","    # Gradient\n","    if grad:\n","      self.last_grad = self.gradient(x)\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, x):\n","    '''Calculate swish function of x'''\n","\n","    if isinstance(x, list):\n","      x = np.array(x)\n","\n","    return x*self.sigmoid(x)\n","\n","  def calculate(self, x, grad: bool = True):\n","    '''Calculate swish function of x'''\n","\n","    # Calculation\n","    y = self(x)\n","\n","    # Update last call data\n","    self._update_last_call(x, y, grad)\n","    \n","    return y\n","\n","  def copy(self):\n","    return Swish()\n","\n","  def gradient(self, x):\n","    '''Calculate swish gradient within x'''\n","\n","    return self.sigmoid(x)*(1 + x*(1 - self.sigmoid(x)))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZgNtLEEYMUI","executionInfo":{"status":"ok","timestamp":1632524649724,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class Relu(object):\n","  '''\n","  Relu Activation Function\n","  \n","    f(x) =  \n","      0 if x < 0\n","      x if x >= 0\n","  '''\n","\n","  def __init__(self):\n","\n","    # Last call data\n","    self.last_grad = 0\n","    self.last_input = 0\n","    self.last_output = 0\n","\n","  def _update_last_call(self, x, y, grad = True):\n","    '''Update last call data'''\n","\n","    # Gradient\n","    if grad:\n","      self.last_grad = self.gradient(x)\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, x):\n","    '''Calculate relu function of x'''\n","\n","    return np.where(x < 0, 0.0, x)\n","\n","  def calculate(self, x, grad: bool = True):\n","    '''Calculate relu function of x'''\n","\n","    # Calculation\n","    y = self(x)\n","\n","    # Update last call data\n","    self._update_last_call(x, y, grad)\n","    \n","    return y\n","\n","  def copy(self):\n","    return Relu()\n","\n","  def gradient(self, x):\n","    '''Calculate relu gradient within x'''\n","\n","    return np.where(x < 0, 0.0, 1)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"KoCUeZ19j6HP","executionInfo":{"status":"ok","timestamp":1632524650274,"user_tz":180,"elapsed":557,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class Tanh(object):\n","  '''\n","  Tanh Activation Function\n","  \n","    f(x) =  tanh(x)\n","  '''\n","\n","  def __init__(self):\n","\n","    # Last call data\n","    self.last_grad = 0\n","    self.last_input = 0\n","    self.last_output = 0\n","\n","  def _update_last_call(self, x, y, grad = True):\n","    '''Update last call data'''\n","\n","    # Gradient\n","    if grad:\n","      self.last_grad = self.gradient(x)\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, x):\n","    '''Calculate sigmoid function of x'''\n","\n","    if isinstance(x, list):\n","      x = np.array(x)\n","\n","    return np.tanh(x)\n","\n","  def calculate(self, x, grad: bool = True):\n","    '''Calculate tanh function of x'''\n","\n","    # Calculation\n","    y = self(x)\n","\n","    # Update last call data\n","    self._update_last_call(x, y, grad)\n","\n","    return y\n","\n","  def copy(self):\n","    return Tanh()\n","\n","  def gradient(self, x):\n","    '''Calculate sigmoid gradient within x'''\n","\n","    return 1 - np.tanh(x)**2"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0amOfaScqyA"},"source":["### Loss Function"]},{"cell_type":"code","metadata":{"id":"_ojfZWi4cscX","executionInfo":{"status":"ok","timestamp":1632524650274,"user_tz":180,"elapsed":7,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class MSE(object):\n","  '''\n","  Mean Squared Error Loss Function\n","  \n","    f(x) =  1/(2*m) * sum((ref_y - hyp_y)**2)\n","  '''\n","\n","  def __init__(self):\n","    \n","    # Last call data\n","    self.last_grad = 0\n","    self.last_input = 0\n","    self.last_output = 0\n","\n","  def _update_last_call(self, x, y, grad = True):\n","    '''Update last call data'''\n","\n","    # Gradient\n","    if grad:\n","      self.last_grad = self.gradient(x[0], x[1])\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, ref: np.ndarray, hyp: np.ndarray):\n","    '''Calculate mean squared error between ref and hyp'''\n","\n","    if isinstance(ref, list):\n","      ref = np.array(ref)\n","    if isinstance(hyp, list):\n","      hyp = np.array(hyp)\n","\n","    size = len(ref)\n","\n","    # Quadratic Error Sum\n","    quadratic_sum = np.sum((ref - hyp)**2)\n","\n","    return quadratic_sum/(2*size)\n","\n","  def calculate(self, ref: np.ndarray, hyp: np.ndarray, grad: bool = True):\n","    '''Calculate mean squared error between ref and hyp'''\n","\n","    # Calculation\n","    y = self(ref, hyp)\n","\n","    # Update last call data\n","    self._update_last_call((ref, hyp), y, grad)\n","\n","    return y\n","\n","  def copy(self):\n","    return MSE()\n","\n","  def gradient(self, ref: np.ndarray, hyp: np.ndarray):\n","    '''Calculate MSE gradient within hyp'''\n","\n","    size = len(ref)\n","\n","    return -np.sum(ref - hyp)/size"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vO1xJRxmHpk"},"source":["### Metrics Function"]},{"cell_type":"code","metadata":{"id":"Mm7bK0eomUp1","executionInfo":{"status":"ok","timestamp":1632524650274,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class MultiClassAccuracy(object):\n","  '''\n","  MultiClass Accuracy Score\n","  '''\n","\n","  def __init__(self):\n","    \n","    self.func = accuracy_score\n","\n","  def one_hot_decode(y):\n","    '''One hot decode y'''\n","\n","    return np.argmax(y, axis=1)\n","\n","  def __call__(self, ref: np.ndarray, hyp: np.ndarray):\n","    '''Apply one hot decode and calculate accuracy ref and hyp'''\n","\n","    if isinstance(ref, list):\n","      ref = np.array(ref)\n","    if isinstance(hyp, list):\n","      hyp = np.array(hyp)\n","\n","    # One hot decode\n","    y_true = one_hot_decode(ref)\n","    y_pred = one_hot_decode(hyp)\n","\n","    # Calculate score\n","    score = self.func(y_true, y_pred)\n","\n","    return score\n","\n","  def calculate(self, ref: np.ndarray, hyp: np.ndarray):\n","    '''Calculate mean squared error between ref and hyp'''\n","\n","    # Calculation\n","    y = self(ref, hyp)\n","\n","    return y\n","\n","  def copy(self):\n","    return MultiClassAccuracy()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ek9SZGMvMrNk"},"source":["### Kernel Function"]},{"cell_type":"code","metadata":{"id":"xlhpQ3wtMuur","executionInfo":{"status":"ok","timestamp":1632524650274,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class KernelGaussian(object):\n","  '''\n","  Kernel Gaussiano function\n","  '''\n","\n","  def __init__(self, centroids, sigma = 1):\n","\n","    # Save parameters\n","    self.centroids = centroids\n","    self.sigma = sigma\n","\n","  def _update_last_call(self, x, y):\n","    '''Update last call data'''\n","\n","    # Update last input and output\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def __call__(self, X: np.ndarray):\n","    '''Apply kernel to data\n","    \n","      Params:\n","        X (np.ndarray): array of data\n","    '''\n","\n","    # Calculate gaussian distance (step 1)\n","    dists = np.sqrt( np.sum( (X-self.centroids.T)**2, axis=1 ) )\n","\n","    # Calculate gaussian distance (step 2)\n","    output = np.exp(-dists**2 / (2 * self.sigma**2))\n","\n","    # Add batch dimention\n","    output = np.expand_dims(output, 0)\n","\n","    # Save last call\n","    self._update_last_call(X, output)\n","\n","    return output\n","\n","  def copy(self):\n","    return KernelGaussian()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6ZGiQnXy3bi"},"source":["### Kmeans"]},{"cell_type":"code","metadata":{"id":"t1O5WJ_by5Yk","executionInfo":{"status":"ok","timestamp":1632524650275,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["def get_centroids(X, n_centroids: int = 2):\n","  '''\n","  Returns centroids of X with kmeans\n","  '''\n","\n","  # Fit kmeans\n","  kmeans = KMeans(n_clusters = n_centroids).fit(X)\n","\n","  # Get centroids\n","  centroids = kmeans.cluster_centers_\n","\n","  # Return centroids\n","  return centroids"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CG3q9s-gsqct"},"source":["### Perceptron Class"]},{"cell_type":"code","metadata":{"id":"H8icU1F4ssXA","executionInfo":{"status":"ok","timestamp":1632524650275,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class Perceptron():\n","\n","  def __init__(self, input_size: int = 2, init_rule: str = 'zero', activation = None):\n","    '''Initializes Perceptron\n","    \n","      Params:\n","        input_size (int): size of input data\n","        init_rule (str): initialization parameter to set initial weight values\n","        activation (func): function to apply activation step\n","      '''\n","\n","    # Perceptron size\n","    self.input_size = input_size\n","    self.size = input_size + 1\n","\n","    # Initialize weights\n","    self.init_weights(init_rule)\n","\n","    # Activation function\n","    if activation is None:\n","      activation = Sigmoid()\n","    self.activation = activation.copy()\n","\n","    # Health check\n","    self._health_check()\n","  \n","  def _health_check(self):\n","    '''Perform a health check prediction'''\n","\n","    # 1D (input_size)\n","    X = np.random.rand(self.input_size)\n","    self(X)\n","\n","    # 2D (4 items, input_size)\n","    X = np.random.rand(4, self.input_size)\n","    self(X)\n","\n","  def init_weights(self, init_rule: str = 'zero'):\n","    '''Initialize weights\n","    \n","      Params:\n","        init_rule (str): initialization parameter to set initial weight values\n","    '''\n","\n","    # Assure initializarion rule\n","    assert init_rule in ['zero', 'rand'], \"'init_rule' must be zero or rand\"\n","\n","    if init_rule == 'zero':\n","      # Zero values\n","      self.weights = np.zeros(self.size)\n","    \n","    elif init_rule == 'rand':\n","      # Random values in [-0.1, 0,1]\n","      self.weights = np.random.rand(self.size) - 0.5\n","      self.weights /= 0.5\n","\n","  def set_weights(self, weights: np.ndarray):\n","    '''Update weights\n","\n","      Params:\n","        weights (np.ndarray): array of weights\n","    '''\n","\n","    # Assure perceptron size equals given weights size\n","    assert self.size == len(weights), f\"Perceptron size ({self.size}) != weights size ({len(weights)})\"\n","\n","    # Update weights\n","    self.weights = weights.copy()\n","\n","  def get_weights(self) -> np.ndarray:\n","    '''Return a copy of current weights'''\n","\n","    # Return weights\n","    return self.weights.copy()\n","\n","  def _add_bias_term(self, X: np.ndarray, value = 1.0, is_batch = True) -> np.ndarray:\n","    '''Add bias term to X values, ie:\n","       Given (x_i) in = [1, 0] -> out: [value, 1, 0]\n","\n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        value (float): Bias factor value. Default = 1.0\n","        is_batch (bool): Boolean to explicity that is or not a batch of items\n","\n","      Returns:\n","        X (np.ndarray) with the bias term concatenated, eg:\n","    '''\n","\n","    if is_batch:\n","      # Get batch_size\n","      batch_size = X.shape[0]\n","\n","      # Add bias term\n","      X = np.concatenate([value*np.ones((batch_size, 1)), X], axis=1)\n","    \n","    else:\n","      # Add bias term\n","      X = np.concatenate([[value], X])\n","\n","    return X\n","\n","  def _prepare_input(self, X: np.ndarray) -> np.ndarray:\n","    '''Prepare input X\n","       \n","       1. Add batch dimension (if applies)\n","       2. Add bias term\n","\n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","    '''\n","\n","    # Make sure is a np.ndarray\n","    X = np.array(X)\n","\n","    # Verify if it is sigle item and batch it\n","    if len(X.shape) == 1:\n","      # Add batch dimension\n","      X = np.expand_dims(X, 0)\n","\n","    # Add bias term\n","    X = self._add_bias_term(X)\n","\n","    return X\n","\n","\n","  def forward(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","    \n","    # Prepare input data\n","    X = self._prepare_input(X)\n","\n","    # Inner product of inputs and weigths (net)\n","    net = self.weights * X # multiplication\n","    net = np.sum(net, axis = 1) # sum reduction\n","\n","    # Apply activation\n","    output = self.activation.calculate(net)\n","\n","    return output\n","\n","  def __call__(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction (runs forward method)\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    return self.forward(X)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ohg2D7-AziyD"},"source":["### Perceptron Layer Class"]},{"cell_type":"code","metadata":{"id":"mYBuCiG_zkhr","executionInfo":{"status":"ok","timestamp":1632524650275,"user_tz":180,"elapsed":5,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class PerceptronLayer():\n","\n","  def __init__(self, input_size: int = 2, units: int = 2, init_rule: str = 'zero',\n","               activation = None):\n","    '''Initializes Perceptron Layer\n","    \n","      Params:\n","        input_size (int): size of input data\n","        units (int): perceptron units to use in layer\n","        init_rule (str): initialization parameter to set initial weight values\n","        activation (func): function to apply activation step\n","      '''\n","\n","    # Perceptron Layer sizes\n","    self.input_size = input_size\n","    self.size = input_size + 1\n","    self.num_units = units\n","    self.output_size = self.num_units\n","\n","    # Activation function\n","    if activation is None:\n","      activation = Sigmoid()\n","    self.activation = activation\n","\n","    # Initialization rule\n","    self.init_rule = init_rule\n","\n","    # Initialize units\n","    self._init_units()\n","\n","    # Health check\n","    self._health_check()\n","\n","  def _save_foward_transform(self, x, y):\n","    self.last_input = x\n","    self.last_output = y\n","\n","  def _init_units(self):\n","    '''Initialize units of Layer'''\n","\n","    # Layer list\n","    self.units = []\n","\n","    # Iterate of amount of units\n","    for unit_id in range(self.num_units):\n","      self.units.append(\n","          Perceptron(\n","              input_size=self.input_size,\n","              init_rule=self.init_rule,\n","              activation=self.activation\n","          )\n","      )\n","  \n","  def _health_check(self):\n","    '''Perform a health check prediction'''\n","\n","    # 1D (input_size)\n","    X = np.random.rand(self.input_size)\n","    self(X)\n","\n","    # 2D (4 items, input_size)\n","    X = np.random.rand(4, self.input_size)\n","    self(X)\n","\n","\n","  def set_weights(self, weights: np.ndarray):\n","    '''Update weights per unit\n","\n","      Params:\n","        weights (np.ndarray): array of weights; size: (num_units, len(unit.weights))\n","    '''\n","\n","    # Assure weights have same length as number of units\n","    assert len(weights) == self.num_units, \"Array of weigths must have shape (num_units, len(unit.weights))\"\n","\n","    for unit_weights, unit in zip(weights, self.units):\n","\n","      # Update weights for each unit\n","      unit.set_weights(unit_weights)\n","\n","  def get_weights(self) -> np.ndarray:\n","    '''Return a array of a copy of current weights per unit'''\n","\n","    units_weights = []\n","\n","    for unit in self.units:\n","\n","      # Append unit weights\n","      units_weights.append(unit.get_weights())\n","\n","    # Return weights\n","    return np.array(units_weights)\n","\n","  def backward(self, delta, value):\n","    '''\n","    Apply backward propagation of the layer\n","    '''\n","\n","    # Get gradient\n","    grad = self.activation.gradient(value)\n","\n","    # Return new delta\n","    new_delta = (delta @ self.get_weights())[:, 1:] * grad\n","\n","    return new_delta\n","\n","  def forward(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","    \n","    # Initalizate logits list (size of num units)\n","    logits = []\n","\n","    for unit in self.units:\n","      \n","      # Apply foward in perceptron unit\n","      logit = unit(X)\n","\n","      # Save logit\n","      logits.append(logit)\n","\n","    # Return transpose logits\n","    logits = np.array(logits).T\n","\n","    return logits\n","\n","  def __call__(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction (runs forward method)\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    y = self.forward(X)\n","\n","    self._save_foward_transform(X, y)\n","\n","    return y"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5Y58Hcm7bqO"},"source":["### MultiLayerPerceptron Class"]},{"cell_type":"code","metadata":{"id":"GYyLmMY64mrq","executionInfo":{"status":"ok","timestamp":1632524650276,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class MultiLayerPerceptron():\n","\n","  def __init__(self, layers: list, loss_func = None, metric = None):\n","    '''Initializes MultiLayerPerceptron Model\n","    \n","      Params:\n","        layers (list): list of perceptron layers\n","        loss_func (func): loss function to be applied\n","      '''\n","\n","    # Perceptron Layers\n","    self.layers = layers\n","    self.num_layers = len(layers)\n","\n","    # Input and output sizes\n","    self.input_size = layers[0].input_size\n","    self.output_size = layers[-1].output_size\n","\n","    # Save loss\n","    if loss_func is None:\n","      self.loss_func = MSE()\n","    else:\n","      self.loss_func = loss_func\n","\n","    # Save metric\n","    self.metric = metric\n","\n","    # Health check\n","    self._health_check()\n","\n","    # Last dWs (weight variations)\n","    self.last_dWs = {}\n","  \n","  def _health_check(self):\n","    '''Perform a health check prediction'''\n","\n","    # 1D (input_size)\n","    X = np.random.rand(self.input_size)\n","    self(X)\n","\n","    # 2D (4 items, input_size)\n","    X = np.random.rand(4, self.input_size)\n","    self(X)\n","\n","  def _extend(self, vec):\n","    \n","    return np.hstack([np.ones((vec.shape[0], 1)), vec])\n","\n","  def _backpropagate(self, x_input: np.ndarray, predicted_y: np.ndarray, \n","                     reference_y: np.ndarray, learning_rate: float, momentum_rate: float):\n","    '''\n","    Backpropagate loss to update each perceptron set of weigths in\n","    each layer of the model\n","\n","    Params:\n","      x_input (np.ndarray): batch of input x\n","      reference_y (np.ndarray): batch of reference y's\n","      predicted_y (np.ndarray): batch of predicted y's\n","      learning_rate (float): learning rate param\n","      momentum_rate (float): momentum rate param\n","    '''\n","\n","    # Get first delta\n","    delta = predicted_y - reference_y\n","\n","    # Get last predicted layer output\n","    last_output = predicted_y\n","\n","    # Initialize weights dict variations\n","    dWs = {}\n","\n","    # Iterate backwards over layers\n","    for i in range(-1, -len(self.layers), -1):\n","      \n","      # Get layer last output\n","      last_output = self.layers[i - 1].last_output\n","\n","      # Update layer weights variation\n","      dWs[i] = delta.T @ self._extend(last_output)\n","\n","      # Get new delta\n","      delta = self.layers[i].backward(delta, last_output)\n","\n","    # Update layer weights variation (first layer)\n","    dWs[-self.num_layers] = delta.T @ self._extend(x_input)\n","\n","    # Initialize current dWs variations\n","    current_dWs = {}\n","\n","    # Update each layer weights\n","    for k, dW in dWs.items():\n","      \n","      # Get current weights\n","      weights = self.layers[k].get_weights()\n","\n","      # Can't apply momentum yet\n","      if self.last_dWs == {}:\n","        variation = -(learning_rate * dW)\n","      \n","      # Apply momentum\n","      else:\n","        variation = -(learning_rate * dW) + momentum_rate * self.last_dWs[k]\n","\n","      # Update weights\n","      weights += variation\n","\n","      # Update weights\n","      self.layers[k].set_weights(weights)\n","\n","      # Update current dWs\n","      current_dWs[k] = variation\n","    \n","    # Update last dWs\n","    self.last_dWs = current_dWs\n","\n","  def get_weights(self) -> np.ndarray:\n","    '''Return a list of arrays of a copy of current weights per layer and unit'''\n","\n","    layer_weights = []\n","\n","    for layer in self.layers:\n","\n","      # Append unit weights\n","      layer_weights.append(layer.get_weights())\n","\n","    # Return weights\n","    return layer_weights\n","\n","  def _get_batch(self, X: np.ndarray, y: np.ndarray, batch_size: int):\n","    '''Generator of batch of items from X and y input data'''\n","\n","    # X and y lengths must match\n","    assert len(X) == len(y), f\"X (len = {len(X)}) and y (len = {len(y)}) lengths must match\"\n","\n","    # Produce batches\n","    batches = []\n","\n","    # For each batch step append items\n","    for step in range(len(X)//batch_size + 2):\n","      \n","      # Get batch\n","      X_batch = X[step*batch_size:(step+1)*batch_size]\n","      y_batch = y[step*batch_size:(step+1)*batch_size]\n","\n","      if len(X_batch) == 0: break\n","\n","      batches.append({\n","          'X': X_batch,\n","          'y': y_batch\n","      })\n","\n","    # Generate each batch pre-computed\n","    for batch in batches:\n","\n","      # Return item\n","      yield batch\n","\n","  def fit(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.1, momentum_rate: float = 1e-5,\n","          max_epochs: int = 5, stop_threshold: float = 1e-3, batch_size: int = 1):\n","    '''\n","    Fit the MLP model using a max_epochs steps, or when the stop_threshold\n","    is met\n","\n","      Params:\n","        X (np.ndarray): a array of inputs, each input must match model input_size (number of features)\n","        y (np.ndarray): a array of target values (labels)\n","        learning_rate (float): hyperparameter to be used on backpropagation\n","        momentum_rate (float): hyperparameter to be used on backpropagation\n","        max_epochs (int): number of maximum epochs to iterate\n","        stop_threshold (float): number to be used to stop training if epoch loss is lower\n","        batch_size (int): size of each batch for the training steps\n","\n","      Returns:\n","        history (dict): A dictionary containing training data over training as epoch loss\n","    '''\n","\n","    # Assure X and y has same size\n","    assert len(X) == len(y), f\"X (len = {len(X)}) and y (len = {len(y)}) lengths must match\"\n","    assert len(y[0]) == self.layers[-1].num_units, f\"y_i (len = {len(y[0])}) and output layer (len = {self.layers[-1].num_units}) lengths must match\"\n","\n","    # Epoch iterator\n","    iterator = tqdm(range(max_epochs), leave=False)\n","\n","    # Num items\n","    num_items = len(y)\n","\n","    # History of train\n","    history = {'loss': [], 'score': []}\n","\n","    for epoch in iterator:\n","\n","      # Log epoch\n","      iterator.set_description(f'Epoch {epoch+1}/{max_epochs}')\n","\n","      # Initialize epoch loss (mean absolute value)\n","      epoch_loss = 0\n","      \n","      # Iterate over all items\n","      for batch in self._get_batch(X, y, batch_size):\n","\n","        # Retriever X and y batch\n","        input_x = batch['X']\n","        reference_y = batch['y']\n","        \n","        # Make prediction\n","        predicted_y = self(input_x)\n","\n","        # Calculate error (loss)\n","        loss = self.loss_func.calculate(reference_y, predicted_y)\n","\n","        # Add to epoch loss\n","        epoch_loss += batch_size * loss/num_items\n","\n","        # Update weights\n","        self._backpropagate(input_x, predicted_y, reference_y, learning_rate, momentum_rate)\n","\n","      # Calculate metric\n","      if self.metric:\n","        score = self.evaluate(X, y)\n","      \n","        # Log epoch loss and metric\n","        iterator.set_postfix({'loss': epoch_loss, 'score': score})\n","        history['score'].append(score)\n","      else:\n","        # Log epoch loss\n","        iterator.set_postfix({'loss': epoch_loss})\n","\n","      # Append history\n","      history['loss'].append(epoch_loss)\n","\n","      # Stop Threshold\n","      if epoch_loss < stop_threshold:\n","        print(f'>> Loss met stop condition (at epoch {epoch+1}): loss = {epoch_loss} < {stop_threshold}')\n","        break\n","\n","    return history\n","\n","  def evaluate(self, X: np.ndarray, y: np.ndarray):\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        y (np.ndarray): Batch of expected predictions (2D array) or a item (1D array)\n","        \n","      Returns:\n","        score (dict)\n","    '''\n","\n","    # Make prediction\n","    y_pred = self(X)\n","\n","    return self.metric(y, y_pred)\n","\n","  def forward(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    # Apply forward on each sequential layer\n","    for layer in self.layers:\n","      X = layer(X)\n","    \n","    return X\n","\n","  def __call__(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction (runs forward method)\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    return self.forward(X)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wu7pus6ZHxEf"},"source":["### RBF Class"]},{"cell_type":"code","metadata":{"id":"zknbGk1pHwio","executionInfo":{"status":"ok","timestamp":1632524650714,"user_tz":180,"elapsed":444,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["class RBFModel():\n","\n","  def __init__(self, input_size, output_size, kernel, loss_func = None, metric = None, init_rule = 'zero'):\n","    '''Initializes RBF Model\n","    \n","      Params:\n","        input_size (int): size of input data\n","        output_size (int): size of output data\n","        loss_func (func): loss function to be applied\n","      '''\n","\n","    # Input and output sizes\n","    self.input_size = input_size\n","    self.size = input_size\n","    self.output_size = output_size\n","\n","    # Save loss\n","    if loss_func is None:\n","      self.loss_func = MSE()\n","    else:\n","      self.loss_func = loss_func\n","\n","    # Save metric\n","    self.metric = metric\n","\n","    # Save kernel function\n","    self.kernel = kernel\n","\n","    # Initialize weights\n","    self.init_weights(init_rule=init_rule)\n","\n","    # Health check\n","    self._health_check()\n","\n","  def init_weights(self, init_rule: str = 'zero'):\n","    '''Initialize weights\n","    \n","      Params:\n","        init_rule (str): initialization parameter to set initial weight values\n","    '''\n","\n","    # Assure initializarion rule\n","    assert init_rule in ['zero', 'rand'], \"'init_rule' must be zero or rand\"\n","\n","    if init_rule == 'zero':\n","      # Zero values\n","      self.weights = np.zeros((self.size, self.output_size))\n","    \n","    elif init_rule == 'rand':\n","      # Random values in [-0.1, 0,1]\n","      self.weights = np.random.random((self.size, self.output_size)) - 0.5\n","      self.weights /= 0.5\n","  \n","  def _health_check(self):\n","    '''Perform a health check prediction'''\n","\n","    # 1D (input_size)\n","    X = np.random.rand(self.input_size)\n","    self(X)\n","\n","    # 2D (1 items, input_size)\n","    X = np.random.rand(1, self.input_size)\n","    self(X)\n","\n","  def set_weights(self, weights: np.ndarray):\n","    '''Update weights\n","\n","      Params:\n","        weights (np.ndarray): array of weights; size: (input_size, output_size)\n","    '''\n","\n","    # Assure weights have same length as number of units\n","    assert weights.shape == self.weights.shape, \"Array of weigths must have shape (input_size, output_size)\"\n","\n","    self.weights = weights.copy()\n","\n","  def get_weights(self) -> np.ndarray:\n","    '''Return a copy of current weights'''\n","\n","    return self.weights.copy()\n","\n","  def _get_batch(self, X: np.ndarray, y: np.ndarray, batch_size: int):\n","    '''Generator of batch of items from X and y input data'''\n","\n","    # X and y lengths must match\n","    assert len(X) == len(y), f\"X (len = {len(X)}) and y (len = {len(y)}) lengths must match\"\n","\n","    # Produce batches\n","    batches = []\n","\n","    # For each batch step append items\n","    for step in range(len(X)//batch_size + 2):\n","      \n","      # Get batch\n","      X_batch = X[step*batch_size:(step+1)*batch_size]\n","      y_batch = y[step*batch_size:(step+1)*batch_size]\n","\n","      if len(X_batch) == 0: break\n","\n","      batches.append({\n","          'X': X_batch,\n","          'y': y_batch\n","      })\n","\n","    # Generate each batch pre-computed\n","    for batch in batches:\n","\n","      # Return item\n","      yield batch\n","\n","  def _fit_weights(self, kernel_output: np.ndarray, predicted_y: np.ndarray, \n","                     reference_y: np.ndarray, learning_rate: float):\n","    '''\n","    Backpropagate loss to update each perceptron set of weigths in\n","    each layer of the model\n","\n","    Params:\n","      kernel_output (np.ndarray): batch of kernel(input_x)\n","      reference_y (np.ndarray): batch of reference y's\n","      predicted_y (np.ndarray): batch of predicted y's\n","      learning_rate (float): learning rate param\n","    '''\n","\n","    # Calculate batch differences\n","    diffs = (reference_y-predicted_y)\n","\n","    # Get batch size\n","    batch_size = kernel_output.shape[0]\n","\n","    # Iterate over batch items\n","    for batch_id in range(batch_size):\n","      \n","      # Get difference\n","      diff = diffs[batch_id].reshape(-1, 1)\n","\n","      # Get kernel item output\n","      kernel_output_item = kernel_output[batch_id].reshape(-1, 1)\n","\n","      # Get current weights\n","      current_weights = self.get_weights()\n","\n","      # Calculate delta weights\n","      delta_weights = learning_rate*(kernel_output_item * diff.T)\n","\n","      # Update weights\n","      self.set_weights(current_weights + delta_weights)\n","\n","  \n","  def fit(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.1, momentum_rate: float = 1e-5,\n","          max_epochs: int = 5, stop_threshold: float = 1e-3, batch_size: int = 1):\n","    '''\n","    Fit the MLP model using a max_epochs steps, or when the stop_threshold\n","    is met\n","\n","      Params:\n","        X (np.ndarray): a array of inputs, each input must match model input_size (number of features)\n","        y (np.ndarray): a array of target values (labels)\n","        learning_rate (float): hyperparameter to be used on backpropagation\n","        momentum_rate (float): hyperparameter to be used on backpropagation\n","        max_epochs (int): number of maximum epochs to iterate\n","        stop_threshold (float): number to be used to stop training if epoch loss is lower\n","        batch_size (int): size of each batch for the training steps\n","\n","      Returns:\n","        history (dict): A dictionary containing training data over training as epoch loss\n","    '''\n","\n","    # Assure X and y has same size\n","    assert len(X) == len(y), f\"X (len = {len(X)}) and y (len = {len(y)}) lengths must match\"\n","    assert len(y[0]) == self.output_size, f\"y_i (len = {len(y[0])}) and output size (len = {self.output_size}) lengths must match\"\n","    assert batch_size == 1, \"Only batch size == 1 fits\"\n","\n","    # Epoch iterator\n","    iterator = tqdm(range(max_epochs), leave=False)\n","\n","    # Num items\n","    num_items = len(y)\n","\n","    # History of train\n","    history = {'loss': [], 'score': []}\n","\n","    for epoch in iterator:\n","\n","      # Log epoch\n","      iterator.set_description(f'Epoch {epoch+1}/{max_epochs}')\n","\n","      # Initialize epoch loss (mean absolute value)\n","      epoch_loss = 0\n","      \n","      # Iterate over all items\n","      for batch in self._get_batch(X, y, batch_size):\n","\n","        # Retriever X and y batch\n","        input_x = batch['X']\n","        reference_y = batch['y']\n","        \n","        # Make prediction\n","        predicted_y = self(input_x)\n","        kernel_output = self.kernel.last_output\n","\n","        # Calculate error (loss)\n","        loss = self.loss_func.calculate(reference_y, predicted_y)\n","\n","        # Add to epoch loss\n","        epoch_loss += batch_size * loss/num_items\n","\n","        # Update weights\n","        self._fit_weights(kernel_output, predicted_y, reference_y, learning_rate)\n","\n","      # Calculate metric\n","      if self.metric:\n","        score = self.evaluate(X, y)\n","      \n","        # Log epoch loss and metric\n","        iterator.set_postfix({'loss': epoch_loss, 'score': score})\n","        history['score'].append(score)\n","      else:\n","        # Log epoch loss\n","        iterator.set_postfix({'loss': epoch_loss})\n","\n","      # Append history\n","      history['loss'].append(epoch_loss)\n","\n","      # Stop Threshold\n","      if epoch_loss < stop_threshold:\n","        print(f'>> Loss met stop condition (at epoch {epoch+1}): loss = {epoch_loss} < {stop_threshold}')\n","        break\n","\n","    return history\n","\n","  def evaluate(self, X: np.ndarray, y: np.ndarray):\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        y (np.ndarray): Batch of expected predictions (2D array) or a item (1D array)\n","        \n","      Returns:\n","        score (dict)\n","    '''\n","\n","    y_pred = []\n","\n","    # Make predictions\n","    for x_item in X: \n","\n","      # Apply forward\n","      pred = self(x_item).reshape(-1)\n","\n","      # Append prediction\n","      y_pred.append(pred)\n","\n","    # Turn to np array\n","    y_pred = np.array(y_pred)\n","\n","    return self.metric(y, y_pred)\n","\n","  def _prepare_input(self, X: np.ndarray) -> np.ndarray:\n","    '''Prepare input X\n","       \n","       1. Add batch dimension (if applies)\n","       2. Add bias term\n","\n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","    '''\n","\n","    # Make sure is a np.ndarray\n","    X = np.array(X)\n","\n","    # Verify if it is sigle item and batch it\n","    if len(X.shape) == 1:\n","      # Add batch dimension\n","      X = np.expand_dims(X, 0)\n","\n","    return X\n","\n","  def forward(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    # Prepare data (add batch dimension if applies)\n","    X = self._prepare_input(X)\n","\n","    # Apply forward\n","    y = self.kernel(X) @ self.weights\n","\n","    return y\n","\n","  def __call__(self, X: np.ndarray) -> np.ndarray:\n","    '''Make a batch or single prediction (runs forward method)\n","    \n","      Params:\n","        X (np.ndarray): Batch of items (2D array) or a item (1D array)\n","        \n","      Returns:\n","        output (np.ndarray) logits\n","    '''\n","\n","    return self.forward(X)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yga2RavVgZOf"},"source":["### Iris Dataset"]},{"cell_type":"markdown","metadata":{"id":"YsAZVWEi0UIS"},"source":["#### Load Dataset"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":55},"id":"PT7yAZm5FdmO","executionInfo":{"status":"ok","timestamp":1632524656658,"user_tz":180,"elapsed":5948,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"aa9570a9-d95b-48c7-921a-02b7ff261b8a"},"source":["from google.colab import files\n","\n","print(\"Por favor faça o upload do arquivo iris.dat\")\n","uploaded = files.upload()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Por favor faça o upload do arquivo iris.dat\n"]},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-3c0ca0fb-5a7e-4d9d-b9d9-2182a32b7703\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3c0ca0fb-5a7e-4d9d-b9d9-2182a32b7703\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHuEROFB7zl6","executionInfo":{"status":"ok","timestamp":1632524672827,"user_tz":180,"elapsed":16178,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"836dec3b-31f6-4c49-c139-4e336f2fcab3"},"source":["############################ Implementação 1 ############################\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn import model_selection\n","from sklearn.cluster import KMeans\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","#loading dataset\n","dataset = pd.read_csv('iris.dat')\n","\n","# Get features\n","X = dataset[['SepalLengthCm',\t'SepalWidthCm',\t'PetalLengthCm',\t'PetalWidthCm']]\n","\n","# Get target class\n","Y = dataset[['Species']]\n","\n","Y = LabelEncoder().fit_transform(Y)\n","\n","#preparating features for newtwork\n","data = np.array( X, dtype=np.float32 )\n","labels = np.array( Y, dtype=np.int32 )\n","\n","#preparating labels for newtwork\n","num_categories = 3\n","new_labels = np.zeros( [ len(labels), num_categories ] )\n","for i in range( len(labels) ):\n","  new_labels[ i, labels[i]-1 ] = 1.\n","labels = new_labels\n","\n","\n","#print features and labels\n","print('---features---')\n","print(data)\n","print('---class---')\n","print(labels)\n","\n","\n","#separating test and training data\n","validation_size = 0.30\n","seed = 7\n","scoring = 'accuracy'\n","X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(data, labels, test_size=validation_size, random_state=seed, shuffle= True)\n","\n","\n","#normalization\n","scaler = MinMaxScaler( (-1,1) )\n","X_train = scaler.fit_transform( X_train )\n","X_validation = scaler.transform( X_validation )\n","print(X_train.shape, X_validation.shape)\n","\n","\n","#RBF NETWORK\n","from sklearn.cluster import KMeans\n","\n","###initialize the weights of the intermediate layer\n","def initialize_centroids( n_centroids, X ):\n","    kmeans = KMeans( n_clusters = n_centroids ).fit(X)\n","    centroids = kmeans.cluster_centers_\n","    return centroids\n","\n","###initialize the weights of the output layer\n","def initialize_weights( n_centroids, n_outputs ):\n","    W = np.random.normal( loc=0, scale=0.1, size=( n_centroids, n_outputs ) )\n","    return W\n","\n","###sigmoid  function\n","def gaussian( C, X, sigma=1. ):\n","    dists = np.sqrt( np.sum( (X-C)**2, axis=1 ) )\n","    return np.exp(-dists**2 / (2 * sigma**2))\n","\n","###step function\n","def step( v ):\n","    if v > 0:\n","        return 1\n","    return 0\n","\n","###forward\n","def forward( C, W, X ):\n","    phi = gaussian( C, X )\n","    V = np.dot( phi, W )\n","    Y = [step(v) for v in V]\n","    Y = np.array( Y )\n","    return Y\n","\n","###prediction\n","def predict( C, W, data ):\n","    outputs = list()\n","    for X in data:\n","        Y = forward( C, W, X )\n","        outputs.append( Y )\n","    return outputs\n","\n","##acurracy\n","def evaluate( C, W, data, t ):\n","    Y = predict( C, W, data )\n","    hits = np.sum( [ np.argmax(Y[i]) == np.argmax(t[i]) for i in range( len(Y) ) ] )\n","    acc = hits / len(Y)\n","    return acc\n","\n","###root-mean-square deviation (RMSD) for each simple\n","def compute_mse( y, t ):\n","  return 1/2 * np.sum( [ (t[i] - y[i])**2 for i in range(len(y)) ] )\n","\n","###root-mean-square deviation (RMSD) for dataset\n","def compute_total_mse( C, W, data, labels ):\n","  y = predict( C, W, data )\n","  E = [ compute_mse( y[i], labels[i] ) for i in range(len(data)) ]\n","  return np.mean( E )\n","\n","###trainning\n","def train( X_train, Y_train, n_centroids, sigma=1.2, eta=0.001, epochs=1000, epsilon=0.1 ):\n","  # Camada intermediária\n","  C = initialize_centroids( n_centroids, X_train )\n","  # Camada de saída\n","  W = initialize_weights( n_centroids, Y_train.shape[1] )\n","  nRows = X_train.shape[0]\n","  error = np.inf\n","  for epoch in range( epochs ):\n","    if error < epsilon:\n","      break\n","    new_W = W\n","    for i in range( nRows ):\n","      Y = forward( C, W, X_train[i] )\n","      for j in range( Y_train.shape[1] ):\n","        new_W[:,j] += eta * gaussian( X_train[i], C, sigma ) * ( Y_train[i,j]-Y[j] )\n","    W = new_W\n","    error = compute_total_mse( C, W, X_train, Y_train )\n","    if not epoch % 200:\n","      print(epoch, error)\n","  return C, W\n","\n","\n","########RESULTS\n","C, W = train( X_train, Y_train, n_centroids=2, sigma=1.0, epochs=1000, epsilon=0.1 )\n","print( 'Train accuracy:', evaluate( C, W, X_train, Y_train ) )\n","print( 'Test accuracy:', evaluate( C, W, X_validation, Y_validation ) )\n","\n","############################ Implementação 1 ############################"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:63: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n"]},{"output_type":"stream","name":"stdout","text":["---features---\n","[[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]\n"," [5.4 3.9 1.7 0.4]\n"," [4.6 3.4 1.4 0.3]\n"," [5.  3.4 1.5 0.2]\n"," [4.4 2.9 1.4 0.2]\n"," [4.9 3.1 1.5 0.1]\n"," [5.4 3.7 1.5 0.2]\n"," [4.8 3.4 1.6 0.2]\n"," [4.8 3.  1.4 0.1]\n"," [4.3 3.  1.1 0.1]\n"," [5.8 4.  1.2 0.2]\n"," [5.7 4.4 1.5 0.4]\n"," [5.4 3.9 1.3 0.4]\n"," [5.1 3.5 1.4 0.3]\n"," [5.7 3.8 1.7 0.3]\n"," [5.1 3.8 1.5 0.3]\n"," [5.4 3.4 1.7 0.2]\n"," [5.1 3.7 1.5 0.4]\n"," [4.6 3.6 1.  0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [4.8 3.4 1.9 0.2]\n"," [5.  3.  1.6 0.2]\n"," [5.  3.4 1.6 0.4]\n"," [5.2 3.5 1.5 0.2]\n"," [5.2 3.4 1.4 0.2]\n"," [4.7 3.2 1.6 0.2]\n"," [4.8 3.1 1.6 0.2]\n"," [5.4 3.4 1.5 0.4]\n"," [5.2 4.1 1.5 0.1]\n"," [5.5 4.2 1.4 0.2]\n"," [4.9 3.1 1.5 0.1]\n"," [5.  3.2 1.2 0.2]\n"," [5.5 3.5 1.3 0.2]\n"," [4.9 3.1 1.5 0.1]\n"," [4.4 3.  1.3 0.2]\n"," [5.1 3.4 1.5 0.2]\n"," [5.  3.5 1.3 0.3]\n"," [4.5 2.3 1.3 0.3]\n"," [4.4 3.2 1.3 0.2]\n"," [5.  3.5 1.6 0.6]\n"," [5.1 3.8 1.9 0.4]\n"," [4.8 3.  1.4 0.3]\n"," [5.1 3.8 1.6 0.2]\n"," [4.6 3.2 1.4 0.2]\n"," [5.3 3.7 1.5 0.2]\n"," [5.  3.3 1.4 0.2]\n"," [7.  3.2 4.7 1.4]\n"," [6.4 3.2 4.5 1.5]\n"," [6.9 3.1 4.9 1.5]\n"," [5.5 2.3 4.  1.3]\n"," [6.5 2.8 4.6 1.5]\n"," [5.7 2.8 4.5 1.3]\n"," [6.3 3.3 4.7 1.6]\n"," [4.9 2.4 3.3 1. ]\n"," [6.6 2.9 4.6 1.3]\n"," [5.2 2.7 3.9 1.4]\n"," [5.  2.  3.5 1. ]\n"," [5.9 3.  4.2 1.5]\n"," [6.  2.2 4.  1. ]\n"," [6.1 2.9 4.7 1.4]\n"," [5.6 2.9 3.6 1.3]\n"," [6.7 3.1 4.4 1.4]\n"," [5.6 3.  4.5 1.5]\n"," [5.8 2.7 4.1 1. ]\n"," [6.2 2.2 4.5 1.5]\n"," [5.6 2.5 3.9 1.1]\n"," [5.9 3.2 4.8 1.8]\n"," [6.1 2.8 4.  1.3]\n"," [6.3 2.5 4.9 1.5]\n"," [6.1 2.8 4.7 1.2]\n"," [6.4 2.9 4.3 1.3]\n"," [6.6 3.  4.4 1.4]\n"," [6.8 2.8 4.8 1.4]\n"," [6.7 3.  5.  1.7]\n"," [6.  2.9 4.5 1.5]\n"," [5.7 2.6 3.5 1. ]\n"," [5.5 2.4 3.8 1.1]\n"," [5.5 2.4 3.7 1. ]\n"," [5.8 2.7 3.9 1.2]\n"," [6.  2.7 5.1 1.6]\n"," [5.4 3.  4.5 1.5]\n"," [6.  3.4 4.5 1.6]\n"," [6.7 3.1 4.7 1.5]\n"," [6.3 2.3 4.4 1.3]\n"," [5.6 3.  4.1 1.3]\n"," [5.5 2.5 4.  1.3]\n"," [5.5 2.6 4.4 1.2]\n"," [6.1 3.  4.6 1.4]\n"," [5.8 2.6 4.  1.2]\n"," [5.  2.3 3.3 1. ]\n"," [5.6 2.7 4.2 1.3]\n"," [5.7 3.  4.2 1.2]\n"," [5.7 2.9 4.2 1.3]\n"," [6.2 2.9 4.3 1.3]\n"," [5.1 2.5 3.  1.1]\n"," [5.7 2.8 4.1 1.3]\n"," [6.3 3.3 6.  2.5]\n"," [5.8 2.7 5.1 1.9]\n"," [7.1 3.  5.9 2.1]\n"," [6.3 2.9 5.6 1.8]\n"," [6.5 3.  5.8 2.2]\n"," [7.6 3.  6.6 2.1]\n"," [4.9 2.5 4.5 1.7]\n"," [7.3 2.9 6.3 1.8]\n"," [6.7 2.5 5.8 1.8]\n"," [7.2 3.6 6.1 2.5]\n"," [6.5 3.2 5.1 2. ]\n"," [6.4 2.7 5.3 1.9]\n"," [6.8 3.  5.5 2.1]\n"," [5.7 2.5 5.  2. ]\n"," [5.8 2.8 5.1 2.4]\n"," [6.4 3.2 5.3 2.3]\n"," [6.5 3.  5.5 1.8]\n"," [7.7 3.8 6.7 2.2]\n"," [7.7 2.6 6.9 2.3]\n"," [6.  2.2 5.  1.5]\n"," [6.9 3.2 5.7 2.3]\n"," [5.6 2.8 4.9 2. ]\n"," [7.7 2.8 6.7 2. ]\n"," [6.3 2.7 4.9 1.8]\n"," [6.7 3.3 5.7 2.1]\n"," [7.2 3.2 6.  1.8]\n"," [6.2 2.8 4.8 1.8]\n"," [6.1 3.  4.9 1.8]\n"," [6.4 2.8 5.6 2.1]\n"," [7.2 3.  5.8 1.6]\n"," [7.4 2.8 6.1 1.9]\n"," [7.9 3.8 6.4 2. ]\n"," [6.4 2.8 5.6 2.2]\n"," [6.3 2.8 5.1 1.5]\n"," [6.1 2.6 5.6 1.4]\n"," [7.7 3.  6.1 2.3]\n"," [6.3 3.4 5.6 2.4]\n"," [6.4 3.1 5.5 1.8]\n"," [6.  3.  4.8 1.8]\n"," [6.9 3.1 5.4 2.1]\n"," [6.7 3.1 5.6 2.4]\n"," [6.9 3.1 5.1 2.3]\n"," [5.8 2.7 5.1 1.9]\n"," [6.8 3.2 5.9 2.3]\n"," [6.7 3.3 5.7 2.5]\n"," [6.7 3.  5.2 2.3]\n"," [6.3 2.5 5.  1.9]\n"," [6.5 3.  5.2 2. ]\n"," [6.2 3.4 5.4 2.3]\n"," [5.9 3.  5.1 1.8]]\n","---class---\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n","(105, 4) (45, 4)\n","0 0.6095238095238096\n","200 0.1619047619047619\n","400 0.2857142857142857\n","600 0.17142857142857143\n","800 0.19523809523809524\n","Train accuracy: 0.6857142857142857\n","Test accuracy: 0.6222222222222222\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DATj8A8Ycqc","executionInfo":{"status":"ok","timestamp":1632524672828,"user_tz":180,"elapsed":22,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"21720640-105c-48bd-b817-43ce994f0648"},"source":["dataset = pd.read_csv('iris.dat')\n","\n","# Get features\n","X = dataset[['SepalLengthCm',\t'SepalWidthCm',\t'PetalLengthCm',\t'PetalWidthCm']]\n","\n","# Get target class\n","y = dataset[['Species']]\n","\n","print('>> X:')\n","print(X.describe())\n","print()\n","print('>> Y:')\n","print(y.describe())"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[">> X:\n","       SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n","count     150.000000    150.000000     150.000000    150.000000\n","mean        5.843333      3.054000       3.758667      1.198667\n","std         0.828066      0.433594       1.764420      0.763161\n","min         4.300000      2.000000       1.000000      0.100000\n","25%         5.100000      2.800000       1.600000      0.300000\n","50%         5.800000      3.000000       4.350000      1.300000\n","75%         6.400000      3.300000       5.100000      1.800000\n","max         7.900000      4.400000       6.900000      2.500000\n","\n",">> Y:\n","            Species\n","count           150\n","unique            3\n","top     Iris-setosa\n","freq             50\n"]}]},{"cell_type":"markdown","metadata":{"id":"yQk6DxT-aX7T"},"source":["#### Preprocess Dataset"]},{"cell_type":"code","metadata":{"id":"6D1My0RXcqBm","executionInfo":{"status":"ok","timestamp":1632524672829,"user_tz":180,"elapsed":19,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["def one_hot_encode(y):\n","  '''\n","  One hot encode y\n","  '''\n","\n","  # Initialize a array of zeros\n","  one_hot_y = np.zeros(shape=(len(y), len(np.unique(y))))\n","\n","  # Iterate over values and set class as 1\n","  for i, y_value in enumerate(y):\n","    one_hot_y[i, y_value] = 1\n","\n","  return one_hot_y\n","\n","def one_hot_decode(y):\n","  '''\n","  One hot decode y\n","  '''\n","\n","  return np.argmax(y, axis=1)\n","\n","def label_encode(y):\n","  '''\n","  Label encode y\n","  '''\n","\n","  # Initialize encoder\n","  encoder = LabelEncoder()\n","\n","  # Encoding\n","  encoded_y = encoder.fit_transform(y)\n","\n","  return encoded_y, encoder\n","\n","def label_decode(y, encoder):\n","  '''\n","  Label decode y\n","  '''\n","\n","  return encoder.inverse_transform(y)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUVr3wjMoy_P"},"source":["##### Label encoder"]},{"cell_type":"code","metadata":{"id":"PXWkO-Pco3lr","executionInfo":{"status":"ok","timestamp":1632524672829,"user_tz":180,"elapsed":18,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["y_encoded, encoder = label_encode(y.values.reshape(-1))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YddsoIFVd5vQ"},"source":["##### One hot / Data split"]},{"cell_type":"code","metadata":{"id":"W5OO6cNCYTUj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632524672829,"user_tz":180,"elapsed":18,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"473b41ed-11f6-4f7b-b445-5977e362ca54"},"source":["# One hot encode y\n","one_hot_y = one_hot_encode(y_encoded)\n","\n","# Divide train/test\n","test_size = 0.2\n","X_train, X_test, Y_train, Y_test = train_test_split(X.values, one_hot_y, test_size = test_size, random_state=SEED)\n","X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((120, 4), (30, 4), (120, 3), (30, 3))"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"zOPuwyeDd9y7"},"source":["##### Scale data"]},{"cell_type":"code","metadata":{"id":"coTXZUfbcFf4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632524672830,"user_tz":180,"elapsed":15,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"704312bf-abb7-4028-fa52-9385df2188d7"},"source":["# Initialize scaler\n","scaler = StandardScaler()\n","\n","# Scale data\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Features\n","IRIS_NUM_FEATURES = X_train_scaled.shape[1]\n","\n","X_train_scaled.shape, X_test_scaled.shape"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((120, 4), (30, 4))"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"6DO0LEch0P8S"},"source":["#### Build Models"]},{"cell_type":"code","metadata":{"id":"pGgHi6mYhFUP","executionInfo":{"status":"ok","timestamp":1632524672830,"user_tz":180,"elapsed":14,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["def build_model(kind = 'mlp', kernel=None):\n","  '''\n","  Return untrained model for Iris Problem\n","  '''\n","\n","  # Set random seed\n","  np.random.seed(SEED)\n","\n","  # Assert kind is 'sigmoid' or 'tanh'\n","  assert kind in ['mlp', 'rbf'], \"kind must be 'mlp' or 'rbf'\"\n","\n","  if kind == 'mlp':\n","    \n","    # MLP model\n","    layer1 = PerceptronLayer(input_size=IRIS_NUM_FEATURES, units=2, init_rule='rand', activation=Swish())\n","    layer2 = PerceptronLayer(input_size=2, units=3, init_rule='rand', activation=Sigmoid())\n","\n","    # Model\n","    model = MultiLayerPerceptron(layers = [layer1, layer2], loss_func=MSE(), metric=MultiClassAccuracy())\n","\n","  elif kind == 'rbf':\n","\n","    assert kernel is not None\n","\n","    # RBG model\n","    model = RBFModel(input_size=4, output_size=3, kernel=kernel, loss_func=MSE(), metric=MultiClassAccuracy(), init_rule='zero')\n","  \n","  # Return model\n","  return model"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"eR3iI3P9zF_S","executionInfo":{"status":"ok","timestamp":1632524672830,"user_tz":180,"elapsed":13,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":["# MLP #\n","model_mlp = build_model(kind='mlp')\n","\n","# RBF #\n","# Get centroids with kmeans\n","centroids = get_centroids(X_train_scaled, n_centroids=4)\n","\n","# Initialize kernel function\n","kernel = KernelGaussian(centroids)\n","\n","# get model\n","model_rbf = build_model(kind='rbf', kernel=kernel)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6AIv9wd0XB1"},"source":["#### Train"]},{"cell_type":"markdown","metadata":{"id":"LClU_bH_q6CF"},"source":["##### MLP"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVrZiX0G0d6A","executionInfo":{"status":"ok","timestamp":1632524672831,"user_tz":180,"elapsed":14,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"209c6612-7160-4fb3-c1d2-2e0de52d90d4"},"source":["# Model accuracy before train:\n","\n","print('>> Accuracy (before on test set)')\n","model_mlp.evaluate(X_test_scaled, Y_test)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Accuracy (before on test set)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.36666666666666664"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["6823deee41c24c1eb533dd59328d2219","abea2ff96a884a6fae096366b62cf212","38129aa21e5a4c6c8a3657df76bfa8ad","4b71f585858e4669a3279fb209a5fa6e","102c27228d1e4d7bb0bbfe1b536da782","1ab403d31fec4378a3f34800fb955434","b8e8a23802074747aa896c2a851f50f0","2bece20782994b26807b3aea716d2582","fd99e512ccab496d878644b291403e36","9331beeeadf5484e91712a70e52eb468","ec19e2e1153b40338634816a24636dbb"]},"id":"poBJAmJWYisF","executionInfo":{"status":"ok","timestamp":1632524697272,"user_tz":180,"elapsed":24453,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"e6e14c8a-20ef-400a-f911-4131da971a00"},"source":["hist_mlp = model_mlp.fit(X_train_scaled, Y_train, max_epochs=500, batch_size=1, learning_rate=0.007, momentum_rate=0.03)"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6823deee41c24c1eb533dd59328d2219","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKAXFljd0lsH","executionInfo":{"status":"ok","timestamp":1632524697273,"user_tz":180,"elapsed":16,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"8ee08008-5d8f-436a-ed97-9ac03d8c9d7e"},"source":["# Model accuracy after train:\n","\n","print('>> Accuracy (after train on test set)')\n","model_mlp.evaluate(X_test_scaled, Y_test)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Accuracy (after train on test set)\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"0x5vI2TEr23_"},"source":["##### RBF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9o0u1tMyr4aU","executionInfo":{"status":"ok","timestamp":1632524697273,"user_tz":180,"elapsed":13,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"b35dcd32-de18-4f39-cb3e-ac5c687fdd52"},"source":["# Model accuracy before train:\n","\n","print('>> Accuracy (before on test set)')\n","model_rbf.evaluate(X_test_scaled, Y_test)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Accuracy (before on test set)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.3333333333333333"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["26ed656db0ca4742a21cf19e2271330a","ecb054a8f99247e8bab3cc763355ca2f","a574482289ea4582a613917fecefe01e","e5cf8b7e03f4443392719a6659db255f","9f8e5e1287ed43019614a71f99f30480","9f78ca637e114fd6b0995b6d19d61655","427f48725d9244189e6515740961822c","682db4a662784225a016d699d6fce63d","f5297f0df4fd4ed3bfa658fe8b96719a","b987d4bf0afa4ff082528f3fe7006995","644a0a11ae454f2dbd4695ce07f30dad"]},"id":"kcdlOJejr4ci","executionInfo":{"status":"ok","timestamp":1632524715432,"user_tz":180,"elapsed":18169,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"14cc1c90-87f2-499d-a176-b158fcbe852c"},"source":["hist_rbf = model_rbf.fit(X_train_scaled, Y_train, max_epochs=1000, batch_size=1, learning_rate=0.003)"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26ed656db0ca4742a21cf19e2271330a","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ip6dmpTr4ew","executionInfo":{"status":"ok","timestamp":1632524715433,"user_tz":180,"elapsed":25,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"8a9243fb-98d9-450b-e582-95d452a019b4"},"source":["# Model accuracy after train:\n","\n","print('>> Accuracy (after train on test set)')\n","model_rbf.evaluate(X_test_scaled, Y_test)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Accuracy (after train on test set)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8666666666666667"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"Ie-kX0Xa0n-I"},"source":["#### Training Visualization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"Qtjvr9DhYiwr","executionInfo":{"status":"ok","timestamp":1632524716513,"user_tz":180,"elapsed":1096,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"c0893493-25ee-408d-875f-02b131b45c59"},"source":["fig = px.line(y=hist_mlp['score'])\n","fig.update_layout(\n","    title = 'Iris | Accuracy vs Epochs - MLP',\n","    xaxis_title = 'Epoch',\n","    yaxis_title = 'Accuracy'\n",")"],"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"baab6173-d8c3-4c5e-8ce3-ac8bf50aa284\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"baab6173-d8c3-4c5e-8ce3-ac8bf50aa284\")) {\n","                    Plotly.newPlot(\n","                        'baab6173-d8c3-4c5e-8ce3-ac8bf50aa284',\n","                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"y=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"xaxis\": \"x\", \"y\": [0.4083333333333333, 0.4666666666666667, 0.5416666666666666, 0.5916666666666667, 0.6416666666666667, 0.65, 0.65, 0.65, 0.65, 0.725, 0.75, 0.775, 0.7833333333333333, 0.775, 0.7833333333333333, 0.7916666666666666, 0.775, 0.7833333333333333, 0.7916666666666666, 0.8, 0.8083333333333333, 0.8166666666666667, 0.825, 0.825, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.85, 0.85, 0.8583333333333333, 0.8583333333333333, 0.8666666666666667, 0.8666666666666667, 0.8666666666666667, 0.8666666666666667, 0.875, 0.875, 0.8833333333333333, 0.8833333333333333, 0.8916666666666667, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9166666666666666, 0.925, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9416666666666667, 0.9416666666666667, 0.9583333333333334, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.9583333333333334, 0.95, 0.95, 0.95, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.975, 0.975, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333], \"yaxis\": \"y\"}],\n","                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Iris | Accuracy vs Epochs - MLP\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Accuracy\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('baab6173-d8c3-4c5e-8ce3-ac8bf50aa284');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"47tOqKIboHHh","executionInfo":{"status":"ok","timestamp":1632524717392,"user_tz":180,"elapsed":885,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}},"outputId":"b612d99f-295f-4188-ee30-183e84ce773f"},"source":["fig = px.line(y=hist_rbf['score'])\n","fig.update_layout(\n","    title = 'Iris | Accuracy vs Epochs - RBF',\n","    xaxis_title = 'Epoch',\n","    yaxis_title = 'Accuracy'\n",")"],"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"782e85c1-84f4-4d9c-8b34-b34353b6c2f7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"782e85c1-84f4-4d9c-8b34-b34353b6c2f7\")) {\n","                    Plotly.newPlot(\n","                        '782e85c1-84f4-4d9c-8b34-b34353b6c2f7',\n","                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"y=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"xaxis\": \"x\", \"y\": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6833333333333333, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.6916666666666667, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.7666666666666667, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.7833333333333333, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.8, 0.8, 0.8, 0.8, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667], \"yaxis\": \"y\"}],\n","                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Iris | Accuracy vs Epochs - RBF\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Accuracy\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('782e85c1-84f4-4d9c-8b34-b34353b6c2f7');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"WivydNSuB5OA"},"source":["Conclusão: A rede MLP (1 camada, com 2 unidades) teve uma acurácia de 100% no conjunto de testes, enquanto a rede RBG com kernel Gaussiano teve acurácia de 86%."]},{"cell_type":"code","metadata":{"id":"71KjYZboDNpc","executionInfo":{"status":"ok","timestamp":1632524717393,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Nunes Sequeira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjbpJZX1cXPIHyQ8qs02ItTnmGeGQtbj7gk0GSegA=s64","userId":"06789563524461789788"}}},"source":[""],"execution_count":31,"outputs":[]}]}